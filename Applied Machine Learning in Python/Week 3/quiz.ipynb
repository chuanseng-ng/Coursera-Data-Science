{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Question 1\r\n",
    "# A supervised learning model has been built to predict whether someone is infected with a new strain of a virus. \r\n",
    "# The probability of any one person having the virus is 1%. \r\n",
    "# Using accuracy as a metric, what would be a good choice for a baseline accuracy score that the new model would want to outperform?\r\n",
    "# -> 0.99"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Question 2\r\n",
    "# Given the following confusion matrix:\r\n",
    "# Compute the accuracy to three decimal places.\r\n",
    "# -> 0.906"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Question 3\r\n",
    "# Given the following confusion matrix:\r\n",
    "# Compute the precision to three decimal places.\r\n",
    "# -> 0.923"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Question 4\r\n",
    "# Given the following confusion matrix:\r\n",
    "# Compute the recall to three decimal places.\r\n",
    "# -> 0.960"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Question 5\r\n",
    "# Using the fitted model `m` create a precision-recall curve to answer the following question:\r\n",
    "# For the fitted model `m`, approximately what precision can we expect for a recall of 0.8? \r\n",
    "# (Use y_test and X_test to compute the precision-recall curve. \r\n",
    "# If you wish to view a plot, you can use `plt.show()` )\r\n",
    "# -> pre,rec,_ = precision_recall_curve(y_test,m.predict(X_test))\r\n",
    "# -> plt.plot(rec,pre)\r\n",
    "# -> plt.xlabel('Recall')\r\n",
    "# -> plt.ylabel('Precision')\r\n",
    "# -> plt.ylim([0.0, 1.05])\r\n",
    "# -> plt.xlim([0.0, 1.0])\r\n",
    "# -> plt.show()\r\n",
    "# => 0.6"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Question 6\r\n",
    "# Given the following models and AUC scores, match each model to its corresponding ROC curve.\r\n",
    "## Model 1 test set AUC score: 0.91\r\n",
    "## Model 2 test set AUC score: 0.50\r\n",
    "## Model 3 test set AUC score: 0.56\r\n",
    "# -> Model 1: Roc 1\r\n",
    "# -> Model 2: Roc 3\r\n",
    "# -> Model 3: Roc 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Question 7\r\n",
    "# Given the following models and accuracy scores, match each model to its corresponding ROC curve.\r\n",
    "## Model 1 test set AUC score: 0.91\r\n",
    "## Model 2 test set AUC score: 0.79\r\n",
    "## Model 3 test set AUC score: 0.72\r\n",
    "# -> Not enough information is given"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Question 8\r\n",
    "# Using the fitted model `m` what is the micro precision score?\r\n",
    "# (Use y_test and X_test to compute the precision score.)\r\n",
    "# -> print(precision_score(y_test,m.predict(X_test), average='micro'))\r\n",
    "# => 0.744"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Question 9\r\n",
    "# Which of the following is true of the R-Squared metric? (Select all that apply)\r\n",
    "# -> The best possible score is 1.0\r\n",
    "# -> The worst possible score is 0.0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Question 10\r\n",
    "# In a future society, a machine is used to predict a crime before it occurs. \r\n",
    "# If you were responsible for tuning this machine, \r\n",
    "# what evaluation metric would you want to maximize to ensure no innocent people (people not about to commit a crime) are imprisoned (where crime is the positive label)?\r\n",
    "# -> Precision"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Question 11\r\n",
    "# Consider the machine from the previous question. \r\n",
    "# If you were responsible for tuning this machine, \r\n",
    "# what evaluation metric would you want to maximize to ensure all criminals (people about to commit a crime) are imprisoned (where crime is the positive label)?\r\n",
    "# -> Recall"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Question 12\r\n",
    "# A classifier is trained on an imbalanced multiclass dataset. After looking at the modelâ€™s precision scores, \r\n",
    "# you find that the micro averaging is much smaller than the macro averaging score. \r\n",
    "# Which of the following is most likely happening?\r\n",
    "# -> The model is probably misclassifying the frequent labels more than the infrequent labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Question 13\r\n",
    "# Using the already defined RBF SVC model `m`, run a grid search on the parameters C and gamma, for values [0.01, 0.1, 1, 10]. \r\n",
    "# The grid search should find the model that best optimizes for recall. How much better is the recall of this model than the precision? \r\n",
    "# (Compute recall - precision to 3 decimal places)\r\n",
    "# (Use y_test and X_test to compute precision and recall.)\r\n",
    "# -> parameters = {'gamma':[0.01, 0.1, 1, 10], 'C':[0.01, 0.1, 1, 10]}\r\n",
    "# -> clf = GridSearchCV(m, parameters, scoring = 'recall')\r\n",
    "# -> y_pred = clf.best_estimator_.predict(X_test)\r\n",
    "# -> rec = recall_score(y_test, y_pred, average='binary')\r\n",
    "# -> pre = precision_score(y_test, y_pred, average='binary')\r\n",
    "# -> print(rec-pre)\r\n",
    "# => 0.52"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Question 14\r\n",
    "# Using the already defined RBF SVC model `m`, run a grid search on the parameters C and gamma, for values [0.01, 0.1, 1, 10]. \r\n",
    "# The grid search should find the model that best optimizes for precision. How much better is the precision of this model than the precision? \r\n",
    "# (Compute recall - precision to 3 decimal places)\r\n",
    "# (Use y_test and X_test to compute precision and recall.)\r\n",
    "# -> parameters = {'gamma':[0.01, 0.1, 1, 10], 'C':[0.01, 0.1, 1, 10]}\r\n",
    "# -> clf = GridSearchCV(m, parameters, scoring = 'precision')\r\n",
    "# -> clf.fit(X_train, y_train)\r\n",
    "# -> y_pred = clf.best_estimator_.predict(X_test)\r\n",
    "# -> rec = recall_score(y_test, y_pred, average='binary')\r\n",
    "# -> pre = precision_score(y_test, y_pred, average='binary')\r\n",
    "# -> print(rec-pre)\r\n",
    "# => 0.15"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}